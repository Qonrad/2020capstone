{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from sklearn import metrics\n",
    "\n",
    "trialOrders = {}\n",
    "\n",
    "import os\n",
    "\n",
    "path = './events'\n",
    "\n",
    "def compute_skourascore(subject_performace, idealized_performance):\n",
    "    return scipy.stats.pearsonr(subject_performace, idealized_performance)[0]\n",
    "\n",
    "def compute_auc_score(counterbalanced_angles, length):\n",
    "    score = metrics.auc(np.arange(length * 1.0), counterbalanced_angles) / metrics.auc(np.arange(length * 1.0), np.full((length, 1), 90))\n",
    "    return score\n",
    "\n",
    "def compute_peak_score(counterbalanced_angles):\n",
    "    return np.amax(counterbalanced_angles)\n",
    "\n",
    "def compute_ttp_score(counterbalanced_angles):\n",
    "    return np.argmax(counterbalanced_angles)\n",
    "\n",
    "def find_empty_times(data):\n",
    "    intermissions = data[data['instruction']==\" Push Button\"].index.tolist()\n",
    "    rests = data[data['instruction']==\" Rest\"]\n",
    "    first_scan_index = data[data['instruction']!=\" Rest\"].index.tolist()[0] - 1\n",
    "    first_rest_at_end = data[data['instruction']!=\" Rest\"].index.tolist()[-1] + 1\n",
    "    times = [first_scan_index] + intermissions + [first_rest_at_end]\n",
    "    return times\n",
    "\n",
    "def determine_trialorder(data, times):\n",
    "    trialOrder = []\n",
    "    for trialnum in range(12):\n",
    "        this_trial = data[(times[trialnum] + 1):times[trialnum + 1]][data['feedback']==\"On\"]\n",
    "        trialOrder += [this_trial['left_text'].tolist()[0][1:] + \"-\" + this_trial['right_text'].tolist()[0][1:], this_trial['instruction'].tolist()[0][1:]]\n",
    "    return trialOrder\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.tsv' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rad/.local/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "means_template = {'ID':[], 'down': [], 'up': [], 'both': []}\n",
    "\n",
    "series_template = {'ID':[]}\n",
    "for i in range(1, 13):\n",
    "    series_template[str(i)] = []\n",
    "\n",
    "downseries_template = {'ID':[]}\n",
    "for i in range(1, 7):\n",
    "    downseries_template[str(i)] = []\n",
    "    \n",
    "upseries_template = {'ID':[]}\n",
    "for i in range(1, 7):\n",
    "    upseries_template[str(i)] = []\n",
    "\n",
    "skouradict = {'means':copy.deepcopy(means_template), 'series':copy.deepcopy(series_template), 'downseries':copy.deepcopy(downseries_template), 'upseries': copy.deepcopy(upseries_template)}\n",
    "aucdict = {'means':copy.deepcopy(means_template), 'series':copy.deepcopy(series_template), 'downseries':copy.deepcopy(downseries_template), 'upseries': copy.deepcopy(upseries_template)}\n",
    "peakdict = {'means':copy.deepcopy(means_template), 'series':copy.deepcopy(series_template), 'downseries':copy.deepcopy(downseries_template), 'upseries': copy.deepcopy(upseries_template)}\n",
    "ttpdict = {'means':copy.deepcopy(means_template), 'series':copy.deepcopy(series_template), 'downseries':copy.deepcopy(downseries_template), 'upseries': copy.deepcopy(upseries_template)}\n",
    "\n",
    "for i in range(len(files)):\n",
    "    #parsing filename to find NKI subject ID\n",
    "    subpos = files[i].find('sub-A')\n",
    "    subjID = files[i][(subpos + 4):(subpos + 13)]\n",
    "    #reading events.tsv file as \"data\"\n",
    "    data = pd.read_csv(files[i], sep=\"\\t\")\n",
    "    times = find_empty_times(data)\n",
    "    trialOrder = determine_trialorder(data, times)\n",
    "    \n",
    "    skouradict['series']['ID'] += [subjID]\n",
    "    skouradict['upseries']['ID'] += [subjID]\n",
    "    skouradict['downseries']['ID'] += [subjID]\n",
    "    \n",
    "    aucdict['series']['ID'] += [subjID]\n",
    "    aucdict['upseries']['ID'] += [subjID]\n",
    "    aucdict['downseries']['ID'] += [subjID]\n",
    "    \n",
    "    peakdict['series']['ID'] += [subjID]\n",
    "    peakdict['upseries']['ID'] += [subjID]\n",
    "    peakdict['downseries']['ID'] += [subjID]\n",
    "    \n",
    "    ttpdict['series']['ID'] += [subjID]\n",
    "    ttpdict['upseries']['ID'] += [subjID]\n",
    "    ttpdict['downseries']['ID'] += [subjID]\n",
    "    \n",
    "    series_position = 1\n",
    "    downseries_position = 1\n",
    "    upseries_position = 1\n",
    "    \n",
    "    for trialnum in range(12):\n",
    "        #this_trial is the data just from the trial of trialnum\n",
    "        #this_trial is NOT set to the FIRST 15 TRs (first 30 seconds) of each trial!\n",
    "        this_trial = data[(times[trialnum] + 1):times[trialnum + 1]]#[0:16]\n",
    "        length = len(this_trial.needle_position.values)\n",
    "        instruction = trialOrder[(trialnum * 2) + 1]\n",
    "        polarity = trialOrder[(trialnum * 2)]\n",
    "        if instruction == \"Focus\":\n",
    "            if polarity == 'Focused-Wandering':\n",
    "                idealized = np.linspace(90, 90 + (length - 1), num=length)\n",
    "                auc_balanced = (this_trial.needle_position.values - 90)\n",
    "            elif polarity == 'Wandering-Focused':\n",
    "                idealized = np.linspace(90, 90 - (length - 1), num=length)\n",
    "                auc_balanced = (this_trial.needle_position.values - 90) * -1\n",
    "            \n",
    "            #calculating scores\n",
    "            auc = compute_auc_score(auc_balanced, length)\n",
    "            skourascore = compute_skourascore(this_trial.needle_position.values, idealized)\n",
    "            peak = compute_peak_score(auc_balanced)\n",
    "            ttp = compute_ttp_score(auc_balanced)\n",
    "            \n",
    "            #storing scores in memory\n",
    "            skouradict['downseries'][str(downseries_position)] += [skourascore]\n",
    "            aucdict['downseries'][str(downseries_position)] += [auc]\n",
    "            peakdict['downseries'][str(downseries_position)] += [peak]\n",
    "            ttpdict['downseries'][str(downseries_position)] += [ttp]\n",
    "            \n",
    "            downseries_position += 1\n",
    "        elif instruction == \"Wander\":\n",
    "            if polarity == 'Focused-Wandering':\n",
    "                idealized = np.linspace(90, 90 - (length - 1), num=length)\n",
    "                auc_balanced = (this_trial.needle_position.values - 90) * -1\n",
    "            elif polarity == 'Wandering-Focused':\n",
    "                idealized = np.linspace(90, 90 + (length - 1), num=length)\n",
    "                auc_balanced = (this_trial.needle_position.values - 90)\n",
    "            \n",
    "            #calculating scores\n",
    "            auc = compute_auc_score(auc_balanced, length)\n",
    "            skourascore = compute_skourascore(this_trial.needle_position.values, idealized)\n",
    "            peak = compute_peak_score(auc_balanced)\n",
    "            ttp = compute_ttp_score(auc_balanced)\n",
    "            \n",
    "            #storing scores in memory\n",
    "            skouradict['upseries'][str(upseries_position)] += [skourascore]\n",
    "            aucdict['upseries'][str(upseries_position)] += [auc]\n",
    "            peakdict['upseries'][str(upseries_position)] += [peak]\n",
    "            ttpdict['upseries'][str(upseries_position)] += [ttp]\n",
    "            \n",
    "            upseries_position += 1\n",
    "        else:\n",
    "            print(\"something is horribly wrong\")\n",
    "        skouradict['series'][str(series_position)] += [skourascore]\n",
    "        aucdict['series'][str(series_position)] += [auc]\n",
    "        peakdict['series'][str(series_position)] += [peak]\n",
    "        ttpdict['series'][str(series_position)] += [ttp]\n",
    "        \n",
    "        series_position += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert series dicts into dataframes\n",
    "#first skourascores\n",
    "skouraseries = pd.DataFrame(skouradict['series'])\n",
    "skouradownseries = pd.DataFrame(skouradict['downseries'])\n",
    "skouraupseries = pd.DataFrame(skouradict['upseries'])\n",
    "#now AUC scores\n",
    "aucseries = pd.DataFrame(aucdict['series'])\n",
    "aucdownseries = pd.DataFrame(aucdict['downseries'])\n",
    "aucupseries = pd.DataFrame(aucdict['upseries'])\n",
    "#now peak scores\n",
    "ttpseries = pd.DataFrame(ttpdict['series'])\n",
    "ttpdownseries = pd.DataFrame(ttpdict['downseries'])\n",
    "ttpupseries = pd.DataFrame(ttpdict['upseries'])\n",
    "#now TTP scores\n",
    "peakseries = pd.DataFrame(peakdict['series'])\n",
    "peakdownseries = pd.DataFrame(peakdict['downseries'])\n",
    "peakupseries = pd.DataFrame(peakdict['upseries'])\n",
    "\n",
    "#now sort them by IDs\n",
    "skouraseries.sort_values(by=['ID'])\n",
    "skouradownseries.sort_values(by=['ID'])\n",
    "skouraupseries.sort_values(by=['ID'])\n",
    "aucseries.sort_values(by=['ID'])\n",
    "aucdownseries.sort_values(by=['ID'])\n",
    "aucupseries.sort_values(by=['ID'])\n",
    "ttpseries.sort_values(by=['ID'])\n",
    "ttpdownseries.sort_values(by=['ID'])\n",
    "ttpupseries.sort_values(by=['ID'])\n",
    "peakseries.sort_values(by=['ID'])\n",
    "peakdownseries.sort_values(by=['ID'])\n",
    "peakupseries.sort_values(by=['ID'])\n",
    "None;\n",
    "\n",
    "skouraseries[['ID']].to_csv(\"events_IDs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for calculating skouras-style \"learning\" score. \"learning\" in this context is measured as the average of trials 4-6 minus the average of trials 1-3. This \"learning\" measurement is independent of score-type, and can be calculated for each one.\n",
    "\n",
    "It also calculates the 6-1 learning measure.\n",
    "\n",
    "I define a function for calculating \"learning\" here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function assumes a trial-series of length 6 (either up or down regulation, but not both)\n",
    "def add_learning_measures(series):\n",
    "    series['first_avg'] = series[['1', '2','3']].mean(axis=1)\n",
    "    series['second_avg'] = series[['4', '5','6']].mean(axis=1)\n",
    "    series['skouras_learning'] = series['second_avg'] - series['first_avg']\n",
    "    series['six_minus_one'] = series['6'] - series['1']\n",
    "    series['difference_between_measures'] = series['six_minus_one'] - series['skouras_learning']\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function for plotting learning curves and histograms of arbitrary # of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "def learningcurve(seriesdata, length, scoretype):\n",
    "    x = []\n",
    "    for i in range(length):\n",
    "        x += [i + 1]\n",
    "\n",
    "    series = pd.DataFrame(seriesdata)\n",
    "    series = series.sort_values(by=['ID'])\n",
    "    series = series.to_numpy()\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    diffs = []\n",
    "    for j in range(len(series)):\n",
    "        x=np.asarray(x).astype(np.float)\n",
    "        ax[0].scatter(x, series[j][1:], color='b', alpha=0.1)\n",
    "        diffs += [series[j][6] - series[j][1]]\n",
    "        y = series[j][1:].astype(np.float)\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"r\", alpha=0.2)\n",
    "    ax[1].hist(diffs, 4)\n",
    "    # set ticks and tick labels\n",
    "    ax[0].set_xlim((1, length))\n",
    "    ax[0].set_xticks(x)\n",
    "    ax[0].set_xticklabels(x)\n",
    "\n",
    "    plt.xlabel('Trial Position (not the actual trial number)')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(scoretype + ' learning curves')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def series_histogram(seriesdata, scoretype, length, n_bins):\n",
    "    series = pd.DataFrame(seriesdata)\n",
    "    series = series.sort_values(by=['ID'])\n",
    "\n",
    "    fig, axs = plt.subplots(length, 1, sharey=True, sharex=True)\n",
    "    for trialPos in range(length):\n",
    "        axs[trialPos].hist(series[str(trialPos + 1)].to_list(), bins=n_bins)\n",
    "        axs[trialPos].set_title(scoretype + ' Trial ' + str(trialPos + 1))\n",
    "    plt.xlabel('Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def learning_hist(series, title, dosample=False):\n",
    "    if dosample:\n",
    "        print(title, \"score\")\n",
    "        print(\"Average difference between measures =\", series['difference_between_measures'].mean(axis=0))\n",
    "        print('skouras_learning measure has a variance of', np.var(series[['skouras_learning']].values))\n",
    "        print('6-1 measure has a variance of', np.var(series[['six_minus_one']].values))\n",
    "        print(\"Here is a random sample of 10 subjects.\")\n",
    "        print(series[['skouras_learning','six_minus_one', 'difference_between_measures']].sample(10))\n",
    "    plt.figure()\n",
    "    x = series[['skouras_learning']].values\n",
    "    y = series[['six_minus_one']].values\n",
    "    sns.distplot(x, kde=True, rug=True, label=\"avg(4,5,6)-avg(1,2,3)\")\n",
    "    sns.distplot(y, kde=True, rug=True, label=\"6 - 1\")\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# aucdownserieslearning = add_learning_measures(aucdownseries)\n",
    "# skouradownserieslearning = add_learning_measures(skouradownseries)\n",
    "# ttpdownserieslearning = add_learning_measures(ttpdownseries)\n",
    "# peakdownserieslearning = add_learning_measures(peakdownseries)\n",
    "\n",
    "\n",
    "# learning_hist(aucdownserieslearning, \"AUC\", dosample=True)\n",
    "# learning_hist(skouradownserieslearning, \"Skouras-Score\", dosample=True)\n",
    "# learning_hist(ttpdownserieslearning, \"Time-to-peak\", dosample=True)\n",
    "# learning_hist(peakdownserieslearning, \"Peak\", dosample=True)\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>div.output_scroll { height: 250em; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in age and clinical status csv file made in pmetrics.ipynb.\n",
    "\n",
    "Going to test for interesting correlations with them and the other data.\n",
    "\n",
    "From skouras:\n",
    "    In the control group, age (M = 30.71 years; SD = 7.48; nb = 62) correlated negatively with overall DMN NF performance score (M = 0.195, SD = 0.312) with a moderate association that explained 17% of the variance, r(62)=-0.412, R2 = 0.17, P = 0.0009; Fig. 3B.\n",
    "    \n",
    "My output:\n",
    "    In the control group, age ( M = 32.05501195912154  years; SD = 7.8142617590016386 n = 63 ) correlated negatively with overall DMN NF performance score ( M = 0.19537175723329936 SD = 0.2955934924510536 ) with an association that explained 17.79582649049908 % of the variance, r(63) = -0.4218509984639017 R2 = 0.1779582649049908 P = 0.0005741932845391171\n",
    "    \n",
    "parameters that matter for correlations    \n",
    "-whether I use first 30 secs or whole session\n",
    "\n",
    "-type of scoring method\n",
    "\n",
    "-type of score aggregation method (learning or overall)\n",
    "\n",
    "-whether I remove sleepers\n",
    "\n",
    "-which trials are considered in the aggregation\n",
    "\n",
    "\n",
    "Also, it seems like the weirdness with n values is caused by how there are a few subjects we don't have ages for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import display, HTML\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.diagnostic import lilliefors\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "\n",
    "def lillie(nums, name=\"\"):\n",
    "    ksstat, pval = lilliefors(nums, pvalmethod='table')\n",
    "    print(\"Lilliefors Test \" + name)\n",
    "    print(\"n=\", len(nums), \"D =\", ksstat, \"p=\", pval)\n",
    "    \n",
    "def compare_with_age(othernums, vname=\"\", path=False, exact=False, nosleep=True):\n",
    "    diags_ages = pd.read_csv('./diags_ages.csv')\n",
    "    diags_ages = pd.merge(skouraseries[['ID']], diags_ages, how='inner', on='ID').sort_values(by=['ID']).drop_duplicates(subset=['ID']).reset_index(drop=True)\n",
    "    if nosleep:\n",
    "        diags_ages = diags_ages[diags_ages['NFB3_MRIQ_01'] == 0].sort_values(by=['ID']).reset_index(drop=True)\n",
    "    if not path:\n",
    "        diags_ages_control = diags_ages[diags_ages['DIAG_01#CODE'] == 'V71.09'].sort_values(by=['ID']).reset_index(drop=True)\n",
    "    else:\n",
    "        diags_ages_control = diags_ages[diags_ages['DIAG_01#CODE'] != 'V71.09'].sort_values(by=['ID']).reset_index(drop=True)\n",
    "    \n",
    "    print(\"Comparing ages of control-group with \" + vname)\n",
    "    \n",
    "    if exact:\n",
    "        diags_ages_control = diags_ages_control[['ID', 'AGE_04']]\n",
    "    else:\n",
    "        diags_ages_control = diags_ages_control[['ID', 'bids_age']]\n",
    "    data = pd.merge(othernums, diags_ages_control, how='inner', on='ID').sort_values(by=['ID']).drop_duplicates(subset=['ID']).reset_index(drop=True)\n",
    "    if exact:\n",
    "        ages = data.AGE_04.values\n",
    "    else:\n",
    "        ages = data.bids_age.values\n",
    "    \n",
    "    lillie(data[othernums.columns[1]].values, vname)\n",
    "    \n",
    "    pr, pp = stats.pearsonr(data[othernums.columns[1]].values, ages)\n",
    "    sr, sp = stats.spearmanr(data[othernums.columns[1]].values, ages)\n",
    "    \n",
    "    x, y = pd.Series(ages, name=\"Age\"), pd.Series(data[othernums.columns[1]].values, name=vname)\n",
    "    plt.figure()\n",
    "    ax = sns.regplot(x=x, y=y, label=vname)\n",
    "    plt.legend()\n",
    "    plt.title(vname)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Correlation Tests \" + vname + \" (ran both, since I haven't figured out yet how to check for normal distribution)\")\n",
    "    print(\"Pearson r =\", pr, \"R2 = \", pr ** 2, \"p = \", pp)\n",
    "    print(\"Spearman r =\", sr, \"R2 = \", sr ** 2, \"p = \", sp)\n",
    "\n",
    "    \n",
    "\n",
    "# diags_ages = pd.read_csv('./diags_ages.csv')\n",
    "# diags_ages = pd.merge(aucdownseries[['ID', 'skouras_learning']], diags_ages, how='inner', on='ID').sort_values(by=['ID']).drop_duplicates(subset=['ID']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# diags_ages_control = diags_ages[diags_ages['DIAG_01#CODE'] == 'V71.09'].sort_values(by=['ID']).reset_index(drop=True)\n",
    "# diags_ages_path = diags_ages[diags_ages['DIAG_01#CODE'] != 'V71.09'].sort_values(by=['ID']).reset_index(drop=True)\n",
    "\n",
    "# lillie(diags_ages.AGE_04.values, \"- Ages of all NFB Participants\")\n",
    "# lillie(diags_ages_control.AGE_04.values, \"- Ages of control-group NFB Participants\")\n",
    "# lillie(diags_ages_path.AGE_04.values, \"- Ages of pathological NFB Participants\")\n",
    "\n",
    "# print(\"\\n\\n\\n\\n\")\n",
    "# trials = aucseries.loc[: , \"7\":\"12\"]\n",
    "# aucseries['scores'] = trials.mean(axis=1)\n",
    "# scores_only = aucseries[['ID', 'scores']]\n",
    "# compare_with_age(scores_only, \"AUC overall score\", nosleep=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 48-comparisons analysis (either 30secs or full-trials, can't really do both at the same time)\n",
    "\n",
    "Variables:\n",
    "score_type (AUC & Skouras)\n",
    "sleepers_group (nonsleepers, sleepers, or everyone) & clinical_status_group (control, path, & both)\n",
    "score_aggregation_method (slope, avg2ndhalf, six_minus_one, end_avg_minus_start_avg)\n",
    "\n",
    "2 x 2 x 3 x 4 = 48 combinations, each spearman-correlated with age (sort the results by p-value)\n",
    "\n",
    "How, computationally, is best to do this?\n",
    "\n",
    "Which functions?\n",
    "\n",
    "a\n",
    "in: score_type_str, df with clinical status, age, and sleep info (for a single score_type)| lists of strings for sleep and clinical_status and aggregation_types\n",
    "implementation:\n",
    "    create output dict - ??? structure\n",
    "    for a in aggregations:\n",
    "        if a is this:\n",
    "            calculate it\n",
    "            add to df\n",
    "    temp df\n",
    "    for c in clinical_statuses:\n",
    "        for s in sleep_types:\n",
    "            if its c:\n",
    "                store appropriate subset of input df in temp df\n",
    "                calculate spearman and add results with appropriate descriptor to output dict\n",
    "            \n",
    "out: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "diags_ages = pd.read_csv('./diags_ages.csv')\n",
    "diags_ages = pd.merge(aucdownseries[['ID']], diags_ages, how='inner', on='ID').sort_values(by=['ID']).drop_duplicates(subset=['ID']).reset_index(drop=True)\n",
    "#one subject is lost here cuz we have literally zero assessment data for them, not even age, maybe will try to get age from participants.tsv (but that's annoying from a data-hygiene perspective)\n",
    "\n",
    "\n",
    "\n",
    "def add_slopes(series, length):\n",
    "    if length == 6:\n",
    "        nums = np.asarray(['1','2', '3', '4', '5', '6'])\n",
    "        series['slope'] = series[['1','2', '3', '4', '5', '6']].apply(lambda x: np.polyfit(range(len(series[['1','2', '3', '4', '5', '6']].columns)), x, 1)[0], axis=1)\n",
    "    elif length == 12:\n",
    "        nums = np.asarray(['1','2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'])\n",
    "        series['slope'] = series[['1','2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']].apply(lambda x: np.polyfit(range(len(series[['1','2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']].columns)), x, 1)[0], axis=1)\n",
    "    return series\n",
    "\n",
    "def add_avg2ndhalf(series, length):\n",
    "    if length == 6:\n",
    "        trials = series.loc[: , \"4\":\"6\"]\n",
    "        series['avg2ndhalf'] = trials.mean(axis=1)\n",
    "    elif length == 12:\n",
    "        trials = series.loc[: , \"6\":\"12\"]\n",
    "        series['avg2ndhalf'] = trials.mean(axis=1)\n",
    "    return series\n",
    "\n",
    "def last_minus_first(series, length):\n",
    "    if length == 6:\n",
    "        series['last_minus_first'] = series['6'] - series['1']\n",
    "    elif length == 12:\n",
    "        series['last_minus_first'] = series['12'] - series['1']\n",
    "    return series\n",
    "\n",
    "def end_avg_minus_start_avg(series, length):\n",
    "    if length == 6:\n",
    "        series['end_avg_minus_start_avg'] = series[['4', '5','6']].mean(axis=1) - series[['1', '2','3']].mean(axis=1)\n",
    "    elif length == 12:\n",
    "        series['end_avg_minus_start_avg'] = series[['6', '7', '8', '9', '10', '11', '12']].mean(axis=1) - series[['1','2', '3', '4', '5', '6']].mean(axis=1)\n",
    "    return series\n",
    "\n",
    "def multi_subset_comparison(score_type_name, scoreseries, assessment_data, length=6, aggregation_methods=['slope', 'avg2ndhalf', 'last_minus_first', 'end_avg_minus_start_avg'], clinical_status_groups=['control', 'path', 'control_and_path']):\n",
    "    ultraseries = scoreseries.copy()\n",
    "    \n",
    "    #compute the necessary aggregation values (maybe should move this outside of the function?)\n",
    "    for a in aggregation_methods:\n",
    "        if a == 'slope':\n",
    "            ultraseries = add_slopes(ultraseries, length)\n",
    "        if a == 'avg2ndhalf':\n",
    "            ultraseries = add_avg2ndhalf(ultraseries, length)\n",
    "        if a == 'last_minus_first':\n",
    "            ultraseries = last_minus_first(ultraseries, length)\n",
    "        if a == 'end_avg_minus_start_avg':\n",
    "            ultraseries = end_avg_minus_start_avg(ultraseries, length)\n",
    "    for c in clinical_status_groups:\n",
    "        subset_temp = assessment_data.copy()\n",
    "        if c == 'control':\n",
    "            subset_temp = subset_temp[subset_temp['DIAG_01#CODE'] == 'V71.09'].sort_values(by=['ID']).reset_index(drop=True)\n",
    "        elif c == 'path':\n",
    "            subset_temp = subset_temp[subset_temp['DIAG_01#CODE'] != 'V71.09'].dropna(subset=['DIAG_01#CODE']).sort_values(by=['ID']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        analysis_temp = ultraseries.copy()\n",
    "        analysis_temp = pd.merge(subset_temp, analysis_temp[['ID'] + aggregation_methods], how='inner', on='ID').sort_values(by=['ID']).reset_index(drop=True)\n",
    "        for a in aggregation_methods:\n",
    "            r, p = stats.spearmanr(analysis_temp[a].values, analysis_temp['bids_age'].values)\n",
    "            if p < 0.05:\n",
    "                criteria = score_type_name + ' ' + c + ' ' + a + ' n = ' + str(len(subset_temp.index))\n",
    "                print(criteria)\n",
    "                print(\"Spearman r =\", r, \"R2 = \", r ** 2, \"p = \", p)\n",
    "                \n",
    "    return ultraseries\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent = multi_subset_comparison(\"skouras-down\", skouradownseries, diags_ages, length=6)\n",
    "dependent.set_index('ID', inplace=True)\n",
    "dependent.to_csv(\"./dependent.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
